{"cells":[{"cell_type":"markdown","id":"0fd75ab4-1ea8-40d6-b52c-99bee57053ca","metadata":{"id":"0fd75ab4-1ea8-40d6-b52c-99bee57053ca"},"source":["# Project - Computer Vision - CNN"]},{"cell_type":"markdown","id":"69ce0009-c5a9-426b-b4e1-2d56efc318b8","metadata":{"id":"69ce0009-c5a9-426b-b4e1-2d56efc318b8"},"source":["**Instructions for Students:**\n","\n","Please carefully follow these steps to complete and submit your project:\n","\n","1. **Make a copy of the Project**: Please make a copy of this project either to your own Google Drive or download locally. Work on the copy of the project. The master project is **Read-Only**, meaning you can edit, but it will not be saved when you close the master project. To avoid total loss of your work, remember to make a copy.\n","\n","2. **Completing the Project**: You are required to work on and complete all tasks in the provided project. Be disciplined and ensure that you thoroughly engage with each task.\n","   \n","3. **Creating a Google Drive Folder**: Each of you must create a new folder on your Google Drive. This will be the repository for all your completed project files, aiding you in keeping your work organized and accessible.\n","   \n","4. **Uploading Completed Project**: Upon completion of your project, make sure to upload all necessary files, involving codes, reports, and related documents into the created Google Drive folder. Save this link in the 'Student Identity' section and also provide it as the last parameter in the `submit` function that has been provided.\n","   \n","5. **Sharing Folder Link**: You're required to share the link to your project Google Drive folder. This is crucial for the submission and evaluation of your project.\n","   \n","6. **Setting Permission to Public**: Please make sure your Google Drive folder is set to public. This allows your instructor to access your solutions and assess your work correctly.\n","\n","Adhering to these procedures will facilitate a smooth project evaluation process for you and the reviewers."]},{"cell_type":"markdown","id":"194d8da1-20e4-4b61-b508-8e34c682a480","metadata":{"id":"194d8da1-20e4-4b61-b508-8e34c682a480"},"source":["## Project Description\n","\n","In this CNN Project, you will create your own custom Image Classification. You can collect a dataset of images you are interested in and train a CNN model to differentiate between them. For example, a model could be trained to distinguish between different types of birds, cars, plants, or any other topic of interest."]},{"cell_type":"markdown","id":"602ce873-bfd6-46f9-bf0c-b84b0f9fc90c","metadata":{"id":"602ce873-bfd6-46f9-bf0c-b84b0f9fc90c"},"source":["## Grading Criteria\n","\n","There are 4 tasks with 5 criterias for scoring, all except Criteria 3 have the same weight. Each criteria except Criteria 3 will give you either 100 point if you are correct and 0 if you are wrong. The final score for the project will the the average of all 5 criterias from 4 tasks in this project.\n","\n","* Task-1 Criteria 1: This task will assess your ability to find a good dataset for your model.\n","\n","* Task-2 Criteria 2: This task will assess your ability to create and save your model for later use in the inference step.\n","\n","* Task-2 Criteria 3: The task will assess your ability to evaluate your model based on accuracy score of the model. The accuracy score is directly used as the score. Please refrain from overtraining your model to gain 100% accuracy.\n","\n","* Task-3 Criteria 4: This task will assess your ability to use Gradia as a UI (User Interface).\n","\n","* Task-4 Criteria 5: This task will assess your ability to publish your model to Huggingface.\n"]},{"cell_type":"markdown","id":"93fd012e-8a91-4a27-a43a-27712e705e4f","metadata":{"id":"93fd012e-8a91-4a27-a43a-27712e705e4f"},"source":["## Student Identity"]},{"cell_type":"code","execution_count":1,"id":"7121cf4d-5979-4c79-bf6d-4ac19fd4d032","metadata":{"id":"7121cf4d-5979-4c79-bf6d-4ac19fd4d032","executionInfo":{"status":"ok","timestamp":1727027481042,"user_tz":-480,"elapsed":535,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}}},"outputs":[],"source":["# @title #### Student Identity\n","student_id = \"REAPJWFQ\" # @param {type:\"string\"}\n","name = \"Fathur Imam Mujaddid\" # @param {type:\"string\"}\n","drive_link = \"https://drive.google.com/drive/folders/1PxbknA7csUZcHQFRR4hvLxgtn0B0NuGe?usp=sharing\"  # @param {type:\"string\"}\n","\n","assignment_id = \"00_cnn_project\""]},{"cell_type":"markdown","id":"66c6ca1f-9235-4ced-ab8a-84781febcfa6","metadata":{"id":"66c6ca1f-9235-4ced-ab8a-84781febcfa6"},"source":["## Import Package"]},{"cell_type":"code","execution_count":2,"id":"ce231d87-1657-4f85-a0c9-7ad8aac6e088","metadata":{"id":"ce231d87-1657-4f85-a0c9-7ad8aac6e088","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727027497250,"user_tz":-480,"elapsed":11561,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}},"outputId":"3481107d-8949-4e81-f4ac-426505de3a30"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rggrader\n","  Downloading rggrader-0.1.6-py3-none-any.whl.metadata (485 bytes)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from rggrader) (2.32.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from rggrader) (2.1.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rggrader) (10.4.0)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->rggrader) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->rggrader) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->rggrader) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->rggrader) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->rggrader) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->rggrader) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->rggrader) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->rggrader) (2024.8.30)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->rggrader) (1.16.0)\n","Downloading rggrader-0.1.6-py3-none-any.whl (2.5 kB)\n","Installing collected packages: rggrader\n","Successfully installed rggrader-0.1.6\n"]}],"source":["!pip install rggrader\n","from rggrader import submit, submit_image"]},{"cell_type":"code","execution_count":3,"id":"9dad0abc-34ef-44db-9a88-32d9d8ab2fbe","metadata":{"id":"9dad0abc-34ef-44db-9a88-32d9d8ab2fbe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727027546157,"user_tz":-480,"elapsed":48911,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}},"outputId":"2049864a-424c-4b3f-afb8-3653971089f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gradio\n","  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n","Collecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Collecting fastapi<1.0 (from gradio)\n","  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n","Collecting ffmpy (from gradio)\n","  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting gradio-client==1.3.0 (from gradio)\n","  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting httpx>=0.24.1 (from gradio)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n","Collecting orjson~=3.0 (from gradio)\n","  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart>=0.0.9 (from gradio)\n","  Downloading python_multipart-0.0.10-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n","Collecting ruff>=0.2.2 (from gradio)\n","  Downloading ruff-0.6.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n","Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n","Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n","Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1.0->gradio)\n","  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n","Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n","  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Downloading gradio-4.44.0-py3-none-any.whl (18.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_multipart-0.0.10-py3-none-any.whl (22 kB)\n","Downloading ruff-0.6.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n","Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, h11, ffmpy, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n","Successfully installed aiofiles-23.2.1 fastapi-0.115.0 ffmpy-0.4.0 gradio-4.44.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.10 ruff-0.6.7 semantic-version-2.10.0 starlette-0.38.6 tomlkit-0.12.0 uvicorn-0.30.6 websockets-12.0\n"]}],"source":["#Write any package/module installation that you need here\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import datasets, transforms, models\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","!pip install gradio\n","import gradio as gr\n","from PIL import Image\n"]},{"cell_type":"markdown","id":"bc07a2ba-cefc-425f-b86e-2f554b98bd4f","metadata":{"id":"bc07a2ba-cefc-425f-b86e-2f554b98bd4f"},"source":["## Task-1 Load the dataset\n","\n","In this task, you will prepare and load your dataset. **You can choose any dataset you want**, make sure the data is diverse and large enough to prevent overfitting and improve the model's ability to generalize.\n","\n","If you are using images from the internet, **please respect copyright and privacy laws**. Creative Commons licenses or public domain images are a safe bet, and many APIs (like the Unsplash API) provide access to a large number of such images."]},{"cell_type":"markdown","id":"26150053-ddb0-447a-8def-efab3f75d336","metadata":{"id":"26150053-ddb0-447a-8def-efab3f75d336"},"source":["### 1.1 Optional Custom Dataset\n","Provided below is a custom dataset template that you may want to use for your code. It's completely optional.\n","\n","Alternatively, you can review the material on Data Augmentation or read the Pytorch tutorial https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files"]},{"cell_type":"code","execution_count":4,"id":"c598c9e0-31fe-4c13-956b-49c0033a39e0","metadata":{"id":"c598c9e0-31fe-4c13-956b-49c0033a39e0","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1727027546160,"user_tz":-480,"elapsed":13,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}},"outputId":"aeb7eff0-17a9-43e0-e937-43ad5af05e7f"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Dataset' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-1871a873a14f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCustomImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"]}],"source":["#an example of creating our own custom dataset, you can use this if you want/need. Completely optional.\n","import os\n","import pandas as pd\n","from torchvision.io import read_image\n","\n","class CustomImageDataset(Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = read_image(img_path)\n","        label = self.img_labels.iloc[idx, 1]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label"]},{"cell_type":"markdown","id":"0abe9d36-5f0f-4e71-a24d-c5acea2c3735","metadata":{"id":"0abe9d36-5f0f-4e71-a24d-c5acea2c3735"},"source":["### 1.2 Write your code in the block below\n","\n","In the code block below, prepare and load your dataset. Please include data preprocessing steps such as dividing the dataset into training, validation, and test sets, or data augmentation techniques that you used if any in this section. Do not put the code to build your model here.\n","\n","Some techniques you may use:\n","- Find and load existing dataset from Huggingface or Kaggle. (Easy)\n","- Create your own custom dataset from the images you have in your possesion or internet search and load the dataset. (Hard)\n","- Etc.\n","\n","Hint:\n","- Usually the dataset are loaded into train_dataset and test_dataset"]},{"cell_type":"code","execution_count":5,"id":"ae0e5132-222b-4069-8c67-567b3c71363c","metadata":{"id":"ae0e5132-222b-4069-8c67-567b3c71363c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727027647842,"user_tz":-480,"elapsed":20946,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}},"outputId":"6a9d3ccf-6460-421b-f46b-ce14fd66b44a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Write your data preprocessing code here\n","drive.mount('/content/drive')\n","\n"]},{"cell_type":"code","source":["# Data Augmentation and Transformations\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),        # Augmentation: Random horizontal flip\n","    transforms.RandomRotation(10),            # Augmentation: Random rotation\n","    transforms.ToTensor(),                    # Convert images to tensor\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize as per ImageNet stats\n","])\n","\n","# Load dataset from Google Drive\n","dataset_dir = '/content/drive/MyDrive/17FlowerOxfordDataset'  # Ganti dengan lokasi dataset\n","flower_dataset = datasets.ImageFolder(root=dataset_dir, transform=transform)\n","\n","# Split dataset into training and testing sets (90% training, 10% testing)\n","train_size = int(0.9 * len(flower_dataset))\n","val_size = len(flower_dataset) - train_size\n","train_dataset, test_dataset = random_split(flower_dataset, [train_size, val_size])\n","\n","# Dataloaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"],"metadata":{"id":"eOU90goy3hTV","executionInfo":{"status":"ok","timestamp":1727027650456,"user_tz":-480,"elapsed":2617,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}}},"id":"eOU90goy3hTV","execution_count":6,"outputs":[]},{"cell_type":"markdown","id":"4cc33ddd-a526-4684-a4ba-4ccd423d643a","metadata":{"id":"4cc33ddd-a526-4684-a4ba-4ccd423d643a"},"source":["### 1.3 Submission\n","\n","You'll submit the link to your dataset for Task-1.\n","\n","- If you use existing dataset from Kaggle or Huggingface, then you can put the link to those dataset here.\n","\n","- If you use your own custom dataset, Upload and store the custom dataset in your Google Drive that you shared with us and put the link to the folder containing that dataset here."]},{"cell_type":"code","execution_count":7,"id":"bd11320b-2560-4389-a128-613e28f9b8d5","metadata":{"id":"bd11320b-2560-4389-a128-613e28f9b8d5","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1727027653465,"user_tz":-480,"elapsed":1703,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}},"outputId":"665426b0-13f5-4e8c-98b9-8c85f1621368"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Assignment successfully submitted'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}],"source":["# Submit Method\n","dataset_link = \"https://www.kaggle.com/datasets/cantonioupao/oxford-flower-17categories-labelled\" # Put your model link\n","\n","question_id = \"00_cnn_project_dataset\"\n","submit(student_id, name, assignment_id, dataset_link, question_id, drive_link)"]},{"cell_type":"markdown","id":"be3e5a65-0562-408a-9b9d-738e8efd825e","metadata":{"id":"be3e5a65-0562-408a-9b9d-738e8efd825e"},"source":["## Task-2 Build your model\n","\n","In this task, you will now build and save your model. You can either create your own CNN model or choose any pretrained model that you feel is most appropriate for your dataset."]},{"cell_type":"markdown","id":"3683681f-2bbc-4c9d-9d5b-fb5b86dac054","metadata":{"id":"3683681f-2bbc-4c9d-9d5b-fb5b86dac054"},"source":["### 2.1 Write your code in the block below\n","\n","In the code block below, write the code to **create your model, either from scratch or fine tuning a pretrained model**. You will need to write the code for your model definition, such as the layers used, loss function and optimizer. Please include also the training and validation loops.\n","\n","Make sure you **save your model to a file** and **measure the accuracy of your model**, as this will be submitted for this task.\n","\n","Some techniques you may use:\n","- Use pretrained model. (Easy)\n","- Create a CNN model from scratch. (Hard)\n","- Etc.\n","\n","Hint:\n","- Use GPU in Google Colab, it significantly improves the time taken for training, compared to CPU.\n","- **Google Colab GPU usage for free-tier have a limit**, which is unknown, so I suggest you try out in CPU mode that your code works without error, then use GPU for traininig.\n","- If you are going to upload to Huggingface by using the Transformer Trainer during training, make sure you use the Huggingface method. Refer to Transfer Learning section or read the documentation here: https://huggingface.co/docs/transformers/model_sharing"]},{"cell_type":"code","execution_count":8,"id":"fa2ec9d0-da97-499a-91ce-d689321207cd","metadata":{"id":"fa2ec9d0-da97-499a-91ce-d689321207cd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727027659154,"user_tz":-480,"elapsed":1123,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}},"outputId":"1b97d981-1c7e-4b23-b979-f7e2a79c5b93"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 142MB/s]\n"]}],"source":["#Write your code to build your model here\n","\n","accuracy = 0\n","model = models.resnet18(pretrained=True)\n","\n","\n","model.fc = nn.Linear(model.fc.in_features, 15)\n","\n","# Send model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Define Loss Function and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Learning Rate Scheduler\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","# Define accuracy calculation function\n","def calculate_accuracy(outputs, labels):\n","    _, predicted = torch.max(outputs, 1)\n","    correct = (predicted == labels).sum().item()\n","    total = labels.size(0)\n","    accuracy = 100 * correct / total\n","    return accuracy\n","\n","# Training Function\n","def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=10):\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","        model.train()\n","\n","        for i, (images, labels) in enumerate(train_loader):\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","            accuracy = calculate_accuracy(outputs, labels)\n","            correct += (outputs.argmax(1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","\n","            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}, Accuracy: {100 * correct / total:.2f}%')\n","\n","\n","        # Step the scheduler\n","        scheduler.step()\n","\n","    print(\"Finished Training\")\n","\n","# Evaluation Function\n","def evaluate_model(model, test_loader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","\n","            correct += (outputs.argmax(1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","    accuracy = 100 * correct / total\n","    print(f'Accuracy of the model on the test images: {accuracy:.2f}%')\n","    return accuracy\n","\n","\n"]},{"cell_type":"code","execution_count":9,"id":"cded4e17","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cded4e17","executionInfo":{"status":"ok","timestamp":1727028252261,"user_tz":-480,"elapsed":585665,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}},"outputId":"fcd10703-de4b-43de-fa4d-a1587f246985"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Step [1/39], Loss: 0.0317, Accuracy: 3.12%\n","Epoch [1/10], Step [2/39], Loss: 0.0506, Accuracy: 25.00%\n","Epoch [1/10], Step [3/39], Loss: 0.0640, Accuracy: 38.54%\n","Epoch [1/10], Step [4/39], Loss: 0.0778, Accuracy: 44.53%\n","Epoch [1/10], Step [5/39], Loss: 0.0886, Accuracy: 48.75%\n","Epoch [1/10], Step [6/39], Loss: 0.0999, Accuracy: 51.56%\n","Epoch [1/10], Step [7/39], Loss: 0.1128, Accuracy: 54.46%\n","Epoch [1/10], Step [8/39], Loss: 0.1174, Accuracy: 58.98%\n","Epoch [1/10], Step [9/39], Loss: 0.1243, Accuracy: 60.76%\n","Epoch [1/10], Step [10/39], Loss: 0.1280, Accuracy: 63.44%\n","Epoch [1/10], Step [11/39], Loss: 0.1357, Accuracy: 64.49%\n","Epoch [1/10], Step [12/39], Loss: 0.1402, Accuracy: 66.67%\n","Epoch [1/10], Step [13/39], Loss: 0.1476, Accuracy: 67.07%\n","Epoch [1/10], Step [14/39], Loss: 0.1571, Accuracy: 67.41%\n","Epoch [1/10], Step [15/39], Loss: 0.1625, Accuracy: 67.92%\n","Epoch [1/10], Step [16/39], Loss: 0.1685, Accuracy: 68.75%\n","Epoch [1/10], Step [17/39], Loss: 0.1720, Accuracy: 70.04%\n","Epoch [1/10], Step [18/39], Loss: 0.1788, Accuracy: 70.66%\n","Epoch [1/10], Step [19/39], Loss: 0.1869, Accuracy: 71.05%\n","Epoch [1/10], Step [20/39], Loss: 0.1937, Accuracy: 71.41%\n","Epoch [1/10], Step [21/39], Loss: 0.2063, Accuracy: 70.98%\n","Epoch [1/10], Step [22/39], Loss: 0.2125, Accuracy: 71.59%\n","Epoch [1/10], Step [23/39], Loss: 0.2168, Accuracy: 72.28%\n","Epoch [1/10], Step [24/39], Loss: 0.2228, Accuracy: 72.53%\n","Epoch [1/10], Step [25/39], Loss: 0.2273, Accuracy: 72.75%\n","Epoch [1/10], Step [26/39], Loss: 0.2332, Accuracy: 73.20%\n","Epoch [1/10], Step [27/39], Loss: 0.2356, Accuracy: 74.07%\n","Epoch [1/10], Step [28/39], Loss: 0.2413, Accuracy: 74.44%\n","Epoch [1/10], Step [29/39], Loss: 0.2443, Accuracy: 75.00%\n","Epoch [1/10], Step [30/39], Loss: 0.2491, Accuracy: 75.31%\n","Epoch [1/10], Step [31/39], Loss: 0.2546, Accuracy: 75.50%\n","Epoch [1/10], Step [32/39], Loss: 0.2585, Accuracy: 75.88%\n","Epoch [1/10], Step [33/39], Loss: 0.2659, Accuracy: 76.14%\n","Epoch [1/10], Step [34/39], Loss: 0.2712, Accuracy: 76.38%\n","Epoch [1/10], Step [35/39], Loss: 0.2765, Accuracy: 76.34%\n","Epoch [1/10], Step [36/39], Loss: 0.2813, Accuracy: 76.56%\n","Epoch [1/10], Step [37/39], Loss: 0.2856, Accuracy: 76.86%\n","Epoch [1/10], Step [38/39], Loss: 0.2890, Accuracy: 77.22%\n","Epoch [1/10], Step [39/39], Loss: 0.2916, Accuracy: 77.37%\n","Epoch [2/10], Step [1/39], Loss: 0.0033, Accuracy: 90.62%\n","Epoch [2/10], Step [2/39], Loss: 0.0082, Accuracy: 85.94%\n","Epoch [2/10], Step [3/39], Loss: 0.0104, Accuracy: 87.50%\n","Epoch [2/10], Step [4/39], Loss: 0.0153, Accuracy: 85.94%\n","Epoch [2/10], Step [5/39], Loss: 0.0195, Accuracy: 86.25%\n","Epoch [2/10], Step [6/39], Loss: 0.0243, Accuracy: 86.46%\n","Epoch [2/10], Step [7/39], Loss: 0.0304, Accuracy: 85.27%\n","Epoch [2/10], Step [8/39], Loss: 0.0343, Accuracy: 85.55%\n","Epoch [2/10], Step [9/39], Loss: 0.0391, Accuracy: 85.42%\n","Epoch [2/10], Step [10/39], Loss: 0.0425, Accuracy: 85.94%\n","Epoch [2/10], Step [11/39], Loss: 0.0474, Accuracy: 85.80%\n","Epoch [2/10], Step [12/39], Loss: 0.0519, Accuracy: 86.20%\n","Epoch [2/10], Step [13/39], Loss: 0.0533, Accuracy: 86.78%\n","Epoch [2/10], Step [14/39], Loss: 0.0577, Accuracy: 86.61%\n","Epoch [2/10], Step [15/39], Loss: 0.0608, Accuracy: 86.46%\n","Epoch [2/10], Step [16/39], Loss: 0.0642, Accuracy: 86.13%\n","Epoch [2/10], Step [17/39], Loss: 0.0661, Accuracy: 86.95%\n","Epoch [2/10], Step [18/39], Loss: 0.0685, Accuracy: 86.98%\n","Epoch [2/10], Step [19/39], Loss: 0.0752, Accuracy: 86.68%\n","Epoch [2/10], Step [20/39], Loss: 0.0763, Accuracy: 87.03%\n","Epoch [2/10], Step [21/39], Loss: 0.0819, Accuracy: 87.20%\n","Epoch [2/10], Step [22/39], Loss: 0.0827, Accuracy: 87.78%\n","Epoch [2/10], Step [23/39], Loss: 0.0877, Accuracy: 87.77%\n","Epoch [2/10], Step [24/39], Loss: 0.0909, Accuracy: 87.89%\n","Epoch [2/10], Step [25/39], Loss: 0.1008, Accuracy: 87.38%\n","Epoch [2/10], Step [26/39], Loss: 0.1037, Accuracy: 87.50%\n","Epoch [2/10], Step [27/39], Loss: 0.1068, Accuracy: 87.50%\n","Epoch [2/10], Step [28/39], Loss: 0.1117, Accuracy: 87.50%\n","Epoch [2/10], Step [29/39], Loss: 0.1151, Accuracy: 87.72%\n","Epoch [2/10], Step [30/39], Loss: 0.1171, Accuracy: 87.92%\n","Epoch [2/10], Step [31/39], Loss: 0.1191, Accuracy: 88.21%\n","Epoch [2/10], Step [32/39], Loss: 0.1227, Accuracy: 88.09%\n","Epoch [2/10], Step [33/39], Loss: 0.1260, Accuracy: 88.16%\n","Epoch [2/10], Step [34/39], Loss: 0.1282, Accuracy: 88.24%\n","Epoch [2/10], Step [35/39], Loss: 0.1302, Accuracy: 88.30%\n","Epoch [2/10], Step [36/39], Loss: 0.1320, Accuracy: 88.45%\n","Epoch [2/10], Step [37/39], Loss: 0.1333, Accuracy: 88.68%\n","Epoch [2/10], Step [38/39], Loss: 0.1370, Accuracy: 88.65%\n","Epoch [2/10], Step [39/39], Loss: 0.1393, Accuracy: 88.64%\n","Epoch [3/10], Step [1/39], Loss: 0.0009, Accuracy: 96.88%\n","Epoch [3/10], Step [2/39], Loss: 0.0027, Accuracy: 95.31%\n","Epoch [3/10], Step [3/39], Loss: 0.0070, Accuracy: 92.71%\n","Epoch [3/10], Step [4/39], Loss: 0.0083, Accuracy: 93.75%\n","Epoch [3/10], Step [5/39], Loss: 0.0121, Accuracy: 92.50%\n","Epoch [3/10], Step [6/39], Loss: 0.0141, Accuracy: 92.71%\n","Epoch [3/10], Step [7/39], Loss: 0.0182, Accuracy: 91.52%\n","Epoch [3/10], Step [8/39], Loss: 0.0207, Accuracy: 90.62%\n","Epoch [3/10], Step [9/39], Loss: 0.0251, Accuracy: 90.28%\n","Epoch [3/10], Step [10/39], Loss: 0.0291, Accuracy: 89.69%\n","Epoch [3/10], Step [11/39], Loss: 0.0298, Accuracy: 90.62%\n","Epoch [3/10], Step [12/39], Loss: 0.0324, Accuracy: 90.62%\n","Epoch [3/10], Step [13/39], Loss: 0.0335, Accuracy: 90.87%\n","Epoch [3/10], Step [14/39], Loss: 0.0347, Accuracy: 91.52%\n","Epoch [3/10], Step [15/39], Loss: 0.0364, Accuracy: 91.88%\n","Epoch [3/10], Step [16/39], Loss: 0.0376, Accuracy: 92.38%\n","Epoch [3/10], Step [17/39], Loss: 0.0411, Accuracy: 91.91%\n","Epoch [3/10], Step [18/39], Loss: 0.0433, Accuracy: 91.84%\n","Epoch [3/10], Step [19/39], Loss: 0.0458, Accuracy: 91.78%\n","Epoch [3/10], Step [20/39], Loss: 0.0485, Accuracy: 91.56%\n","Epoch [3/10], Step [21/39], Loss: 0.0514, Accuracy: 91.52%\n","Epoch [3/10], Step [22/39], Loss: 0.0539, Accuracy: 91.62%\n","Epoch [3/10], Step [23/39], Loss: 0.0559, Accuracy: 91.71%\n","Epoch [3/10], Step [24/39], Loss: 0.0576, Accuracy: 91.80%\n","Epoch [3/10], Step [25/39], Loss: 0.0597, Accuracy: 92.00%\n","Epoch [3/10], Step [26/39], Loss: 0.0620, Accuracy: 92.07%\n","Epoch [3/10], Step [27/39], Loss: 0.0648, Accuracy: 92.13%\n","Epoch [3/10], Step [28/39], Loss: 0.0678, Accuracy: 92.08%\n","Epoch [3/10], Step [29/39], Loss: 0.0692, Accuracy: 92.13%\n","Epoch [3/10], Step [30/39], Loss: 0.0737, Accuracy: 91.98%\n","Epoch [3/10], Step [31/39], Loss: 0.0756, Accuracy: 92.04%\n","Epoch [3/10], Step [32/39], Loss: 0.0790, Accuracy: 91.89%\n","Epoch [3/10], Step [33/39], Loss: 0.0825, Accuracy: 91.86%\n","Epoch [3/10], Step [34/39], Loss: 0.0851, Accuracy: 91.91%\n","Epoch [3/10], Step [35/39], Loss: 0.0872, Accuracy: 92.05%\n","Epoch [3/10], Step [36/39], Loss: 0.0895, Accuracy: 92.19%\n","Epoch [3/10], Step [37/39], Loss: 0.0959, Accuracy: 91.81%\n","Epoch [3/10], Step [38/39], Loss: 0.1009, Accuracy: 91.78%\n","Epoch [3/10], Step [39/39], Loss: 0.1033, Accuracy: 91.83%\n","Epoch [4/10], Step [1/39], Loss: 0.0024, Accuracy: 93.75%\n","Epoch [4/10], Step [2/39], Loss: 0.0044, Accuracy: 90.62%\n","Epoch [4/10], Step [3/39], Loss: 0.0100, Accuracy: 86.46%\n","Epoch [4/10], Step [4/39], Loss: 0.0131, Accuracy: 87.50%\n","Epoch [4/10], Step [5/39], Loss: 0.0179, Accuracy: 86.25%\n","Epoch [4/10], Step [6/39], Loss: 0.0182, Accuracy: 88.54%\n","Epoch [4/10], Step [7/39], Loss: 0.0191, Accuracy: 89.73%\n","Epoch [4/10], Step [8/39], Loss: 0.0236, Accuracy: 89.45%\n","Epoch [4/10], Step [9/39], Loss: 0.0296, Accuracy: 88.19%\n","Epoch [4/10], Step [10/39], Loss: 0.0314, Accuracy: 88.75%\n","Epoch [4/10], Step [11/39], Loss: 0.0339, Accuracy: 89.49%\n","Epoch [4/10], Step [12/39], Loss: 0.0386, Accuracy: 88.54%\n","Epoch [4/10], Step [13/39], Loss: 0.0389, Accuracy: 89.42%\n","Epoch [4/10], Step [14/39], Loss: 0.0412, Accuracy: 89.73%\n","Epoch [4/10], Step [15/39], Loss: 0.0438, Accuracy: 89.79%\n","Epoch [4/10], Step [16/39], Loss: 0.0470, Accuracy: 89.84%\n","Epoch [4/10], Step [17/39], Loss: 0.0512, Accuracy: 89.89%\n","Epoch [4/10], Step [18/39], Loss: 0.0533, Accuracy: 89.93%\n","Epoch [4/10], Step [19/39], Loss: 0.0544, Accuracy: 90.30%\n","Epoch [4/10], Step [20/39], Loss: 0.0565, Accuracy: 90.47%\n","Epoch [4/10], Step [21/39], Loss: 0.0609, Accuracy: 90.33%\n","Epoch [4/10], Step [22/39], Loss: 0.0681, Accuracy: 89.35%\n","Epoch [4/10], Step [23/39], Loss: 0.0699, Accuracy: 89.54%\n","Epoch [4/10], Step [24/39], Loss: 0.0722, Accuracy: 89.58%\n","Epoch [4/10], Step [25/39], Loss: 0.0768, Accuracy: 89.38%\n","Epoch [4/10], Step [26/39], Loss: 0.0791, Accuracy: 89.42%\n","Epoch [4/10], Step [27/39], Loss: 0.0819, Accuracy: 89.58%\n","Epoch [4/10], Step [28/39], Loss: 0.0877, Accuracy: 89.29%\n","Epoch [4/10], Step [29/39], Loss: 0.0935, Accuracy: 89.01%\n","Epoch [4/10], Step [30/39], Loss: 0.0950, Accuracy: 89.17%\n","Epoch [4/10], Step [31/39], Loss: 0.0985, Accuracy: 89.01%\n","Epoch [4/10], Step [32/39], Loss: 0.1006, Accuracy: 89.26%\n","Epoch [4/10], Step [33/39], Loss: 0.1041, Accuracy: 89.20%\n","Epoch [4/10], Step [34/39], Loss: 0.1084, Accuracy: 89.06%\n","Epoch [4/10], Step [35/39], Loss: 0.1113, Accuracy: 89.02%\n","Epoch [4/10], Step [36/39], Loss: 0.1122, Accuracy: 89.24%\n","Epoch [4/10], Step [37/39], Loss: 0.1158, Accuracy: 89.27%\n","Epoch [4/10], Step [38/39], Loss: 0.1185, Accuracy: 89.23%\n","Epoch [4/10], Step [39/39], Loss: 0.1263, Accuracy: 89.13%\n","Epoch [5/10], Step [1/39], Loss: 0.0007, Accuracy: 96.88%\n","Epoch [5/10], Step [2/39], Loss: 0.0033, Accuracy: 93.75%\n","Epoch [5/10], Step [3/39], Loss: 0.0073, Accuracy: 90.62%\n","Epoch [5/10], Step [4/39], Loss: 0.0096, Accuracy: 90.62%\n","Epoch [5/10], Step [5/39], Loss: 0.0152, Accuracy: 88.12%\n","Epoch [5/10], Step [6/39], Loss: 0.0158, Accuracy: 90.10%\n","Epoch [5/10], Step [7/39], Loss: 0.0169, Accuracy: 91.07%\n","Epoch [5/10], Step [8/39], Loss: 0.0203, Accuracy: 90.62%\n","Epoch [5/10], Step [9/39], Loss: 0.0246, Accuracy: 89.93%\n","Epoch [5/10], Step [10/39], Loss: 0.0279, Accuracy: 89.69%\n","Epoch [5/10], Step [11/39], Loss: 0.0324, Accuracy: 89.49%\n","Epoch [5/10], Step [12/39], Loss: 0.0360, Accuracy: 89.32%\n","Epoch [5/10], Step [13/39], Loss: 0.0376, Accuracy: 89.66%\n","Epoch [5/10], Step [14/39], Loss: 0.0389, Accuracy: 90.18%\n","Epoch [5/10], Step [15/39], Loss: 0.0405, Accuracy: 90.62%\n","Epoch [5/10], Step [16/39], Loss: 0.0434, Accuracy: 91.02%\n","Epoch [5/10], Step [17/39], Loss: 0.0463, Accuracy: 90.99%\n","Epoch [5/10], Step [18/39], Loss: 0.0476, Accuracy: 91.15%\n","Epoch [5/10], Step [19/39], Loss: 0.0491, Accuracy: 91.28%\n","Epoch [5/10], Step [20/39], Loss: 0.0529, Accuracy: 91.09%\n","Epoch [5/10], Step [21/39], Loss: 0.0543, Accuracy: 91.22%\n","Epoch [5/10], Step [22/39], Loss: 0.0561, Accuracy: 91.05%\n","Epoch [5/10], Step [23/39], Loss: 0.0572, Accuracy: 91.30%\n","Epoch [5/10], Step [24/39], Loss: 0.0586, Accuracy: 91.67%\n","Epoch [5/10], Step [25/39], Loss: 0.0602, Accuracy: 91.75%\n","Epoch [5/10], Step [26/39], Loss: 0.0624, Accuracy: 91.59%\n","Epoch [5/10], Step [27/39], Loss: 0.0651, Accuracy: 91.55%\n","Epoch [5/10], Step [28/39], Loss: 0.0658, Accuracy: 91.85%\n","Epoch [5/10], Step [29/39], Loss: 0.0666, Accuracy: 92.03%\n","Epoch [5/10], Step [30/39], Loss: 0.0676, Accuracy: 92.19%\n","Epoch [5/10], Step [31/39], Loss: 0.0686, Accuracy: 92.34%\n","Epoch [5/10], Step [32/39], Loss: 0.0691, Accuracy: 92.58%\n","Epoch [5/10], Step [33/39], Loss: 0.0704, Accuracy: 92.71%\n","Epoch [5/10], Step [34/39], Loss: 0.0715, Accuracy: 92.83%\n","Epoch [5/10], Step [35/39], Loss: 0.0731, Accuracy: 92.86%\n","Epoch [5/10], Step [36/39], Loss: 0.0740, Accuracy: 93.06%\n","Epoch [5/10], Step [37/39], Loss: 0.0752, Accuracy: 93.07%\n","Epoch [5/10], Step [38/39], Loss: 0.0760, Accuracy: 93.17%\n","Epoch [5/10], Step [39/39], Loss: 0.0850, Accuracy: 92.97%\n","Epoch [6/10], Step [1/39], Loss: 0.0015, Accuracy: 93.75%\n","Epoch [6/10], Step [2/39], Loss: 0.0023, Accuracy: 95.31%\n","Epoch [6/10], Step [3/39], Loss: 0.0029, Accuracy: 95.83%\n","Epoch [6/10], Step [4/39], Loss: 0.0057, Accuracy: 95.31%\n","Epoch [6/10], Step [5/39], Loss: 0.0090, Accuracy: 93.12%\n","Epoch [6/10], Step [6/39], Loss: 0.0112, Accuracy: 93.75%\n","Epoch [6/10], Step [7/39], Loss: 0.0142, Accuracy: 93.30%\n","Epoch [6/10], Step [8/39], Loss: 0.0164, Accuracy: 92.97%\n","Epoch [6/10], Step [9/39], Loss: 0.0195, Accuracy: 93.06%\n","Epoch [6/10], Step [10/39], Loss: 0.0212, Accuracy: 93.12%\n","Epoch [6/10], Step [11/39], Loss: 0.0216, Accuracy: 93.75%\n","Epoch [6/10], Step [12/39], Loss: 0.0232, Accuracy: 94.01%\n","Epoch [6/10], Step [13/39], Loss: 0.0241, Accuracy: 94.47%\n","Epoch [6/10], Step [14/39], Loss: 0.0266, Accuracy: 94.64%\n","Epoch [6/10], Step [15/39], Loss: 0.0288, Accuracy: 94.58%\n","Epoch [6/10], Step [16/39], Loss: 0.0303, Accuracy: 94.34%\n","Epoch [6/10], Step [17/39], Loss: 0.0314, Accuracy: 94.30%\n","Epoch [6/10], Step [18/39], Loss: 0.0340, Accuracy: 94.10%\n","Epoch [6/10], Step [19/39], Loss: 0.0344, Accuracy: 94.41%\n","Epoch [6/10], Step [20/39], Loss: 0.0356, Accuracy: 94.38%\n","Epoch [6/10], Step [21/39], Loss: 0.0379, Accuracy: 94.35%\n","Epoch [6/10], Step [22/39], Loss: 0.0396, Accuracy: 94.32%\n","Epoch [6/10], Step [23/39], Loss: 0.0408, Accuracy: 94.29%\n","Epoch [6/10], Step [24/39], Loss: 0.0423, Accuracy: 94.40%\n","Epoch [6/10], Step [25/39], Loss: 0.0431, Accuracy: 94.50%\n","Epoch [6/10], Step [26/39], Loss: 0.0435, Accuracy: 94.71%\n","Epoch [6/10], Step [27/39], Loss: 0.0455, Accuracy: 94.68%\n","Epoch [6/10], Step [28/39], Loss: 0.0470, Accuracy: 94.64%\n","Epoch [6/10], Step [29/39], Loss: 0.0510, Accuracy: 94.29%\n","Epoch [6/10], Step [30/39], Loss: 0.0519, Accuracy: 94.38%\n","Epoch [6/10], Step [31/39], Loss: 0.0524, Accuracy: 94.56%\n","Epoch [6/10], Step [32/39], Loss: 0.0533, Accuracy: 94.63%\n","Epoch [6/10], Step [33/39], Loss: 0.0552, Accuracy: 94.51%\n","Epoch [6/10], Step [34/39], Loss: 0.0560, Accuracy: 94.58%\n","Epoch [6/10], Step [35/39], Loss: 0.0570, Accuracy: 94.64%\n","Epoch [6/10], Step [36/39], Loss: 0.0585, Accuracy: 94.70%\n","Epoch [6/10], Step [37/39], Loss: 0.0588, Accuracy: 94.85%\n","Epoch [6/10], Step [38/39], Loss: 0.0614, Accuracy: 94.82%\n","Epoch [6/10], Step [39/39], Loss: 0.0675, Accuracy: 94.77%\n","Epoch [7/10], Step [1/39], Loss: 0.0004, Accuracy: 100.00%\n","Epoch [7/10], Step [2/39], Loss: 0.0029, Accuracy: 96.88%\n","Epoch [7/10], Step [3/39], Loss: 0.0033, Accuracy: 97.92%\n","Epoch [7/10], Step [4/39], Loss: 0.0038, Accuracy: 97.66%\n","Epoch [7/10], Step [5/39], Loss: 0.0050, Accuracy: 97.50%\n","Epoch [7/10], Step [6/39], Loss: 0.0064, Accuracy: 97.40%\n","Epoch [7/10], Step [7/39], Loss: 0.0072, Accuracy: 97.32%\n","Epoch [7/10], Step [8/39], Loss: 0.0104, Accuracy: 96.48%\n","Epoch [7/10], Step [9/39], Loss: 0.0127, Accuracy: 96.18%\n","Epoch [7/10], Step [10/39], Loss: 0.0178, Accuracy: 94.38%\n","Epoch [7/10], Step [11/39], Loss: 0.0193, Accuracy: 94.60%\n","Epoch [7/10], Step [12/39], Loss: 0.0203, Accuracy: 94.79%\n","Epoch [7/10], Step [13/39], Loss: 0.0249, Accuracy: 93.99%\n","Epoch [7/10], Step [14/39], Loss: 0.0252, Accuracy: 94.42%\n","Epoch [7/10], Step [15/39], Loss: 0.0285, Accuracy: 94.17%\n","Epoch [7/10], Step [16/39], Loss: 0.0310, Accuracy: 94.14%\n","Epoch [7/10], Step [17/39], Loss: 0.0349, Accuracy: 93.75%\n","Epoch [7/10], Step [18/39], Loss: 0.0381, Accuracy: 93.75%\n","Epoch [7/10], Step [19/39], Loss: 0.0392, Accuracy: 93.91%\n","Epoch [7/10], Step [20/39], Loss: 0.0404, Accuracy: 94.06%\n","Epoch [7/10], Step [21/39], Loss: 0.0439, Accuracy: 94.05%\n","Epoch [7/10], Step [22/39], Loss: 0.0458, Accuracy: 94.18%\n","Epoch [7/10], Step [23/39], Loss: 0.0473, Accuracy: 94.29%\n","Epoch [7/10], Step [24/39], Loss: 0.0502, Accuracy: 94.14%\n","Epoch [7/10], Step [25/39], Loss: 0.0528, Accuracy: 94.00%\n","Epoch [7/10], Step [26/39], Loss: 0.0553, Accuracy: 93.87%\n","Epoch [7/10], Step [27/39], Loss: 0.0570, Accuracy: 93.75%\n","Epoch [7/10], Step [28/39], Loss: 0.0581, Accuracy: 93.97%\n","Epoch [7/10], Step [29/39], Loss: 0.0613, Accuracy: 93.64%\n","Epoch [7/10], Step [30/39], Loss: 0.0617, Accuracy: 93.85%\n","Epoch [7/10], Step [31/39], Loss: 0.0637, Accuracy: 93.85%\n","Epoch [7/10], Step [32/39], Loss: 0.0675, Accuracy: 93.85%\n","Epoch [7/10], Step [33/39], Loss: 0.0683, Accuracy: 94.03%\n","Epoch [7/10], Step [34/39], Loss: 0.0707, Accuracy: 93.93%\n","Epoch [7/10], Step [35/39], Loss: 0.0717, Accuracy: 94.02%\n","Epoch [7/10], Step [36/39], Loss: 0.0733, Accuracy: 94.01%\n","Epoch [7/10], Step [37/39], Loss: 0.0748, Accuracy: 94.00%\n","Epoch [7/10], Step [38/39], Loss: 0.0762, Accuracy: 93.91%\n","Epoch [7/10], Step [39/39], Loss: 0.0792, Accuracy: 93.87%\n","Epoch [8/10], Step [1/39], Loss: 0.0016, Accuracy: 93.75%\n","Epoch [8/10], Step [2/39], Loss: 0.0042, Accuracy: 92.19%\n","Epoch [8/10], Step [3/39], Loss: 0.0046, Accuracy: 94.79%\n","Epoch [8/10], Step [4/39], Loss: 0.0055, Accuracy: 95.31%\n","Epoch [8/10], Step [5/39], Loss: 0.0091, Accuracy: 95.00%\n","Epoch [8/10], Step [6/39], Loss: 0.0100, Accuracy: 94.79%\n","Epoch [8/10], Step [7/39], Loss: 0.0103, Accuracy: 95.54%\n","Epoch [8/10], Step [8/39], Loss: 0.0124, Accuracy: 95.70%\n","Epoch [8/10], Step [9/39], Loss: 0.0156, Accuracy: 95.14%\n","Epoch [8/10], Step [10/39], Loss: 0.0172, Accuracy: 95.31%\n","Epoch [8/10], Step [11/39], Loss: 0.0180, Accuracy: 95.74%\n","Epoch [8/10], Step [12/39], Loss: 0.0197, Accuracy: 95.83%\n","Epoch [8/10], Step [13/39], Loss: 0.0211, Accuracy: 95.67%\n","Epoch [8/10], Step [14/39], Loss: 0.0237, Accuracy: 95.54%\n","Epoch [8/10], Step [15/39], Loss: 0.0257, Accuracy: 95.62%\n","Epoch [8/10], Step [16/39], Loss: 0.0263, Accuracy: 95.70%\n","Epoch [8/10], Step [17/39], Loss: 0.0272, Accuracy: 95.77%\n","Epoch [8/10], Step [18/39], Loss: 0.0278, Accuracy: 95.83%\n","Epoch [8/10], Step [19/39], Loss: 0.0284, Accuracy: 95.89%\n","Epoch [8/10], Step [20/39], Loss: 0.0289, Accuracy: 96.09%\n","Epoch [8/10], Step [21/39], Loss: 0.0308, Accuracy: 95.98%\n","Epoch [8/10], Step [22/39], Loss: 0.0311, Accuracy: 96.16%\n","Epoch [8/10], Step [23/39], Loss: 0.0318, Accuracy: 96.20%\n","Epoch [8/10], Step [24/39], Loss: 0.0339, Accuracy: 96.09%\n","Epoch [8/10], Step [25/39], Loss: 0.0340, Accuracy: 96.25%\n","Epoch [8/10], Step [26/39], Loss: 0.0341, Accuracy: 96.39%\n","Epoch [8/10], Step [27/39], Loss: 0.0343, Accuracy: 96.53%\n","Epoch [8/10], Step [28/39], Loss: 0.0361, Accuracy: 96.43%\n","Epoch [8/10], Step [29/39], Loss: 0.0363, Accuracy: 96.55%\n","Epoch [8/10], Step [30/39], Loss: 0.0370, Accuracy: 96.56%\n","Epoch [8/10], Step [31/39], Loss: 0.0372, Accuracy: 96.67%\n","Epoch [8/10], Step [32/39], Loss: 0.0388, Accuracy: 96.68%\n","Epoch [8/10], Step [33/39], Loss: 0.0394, Accuracy: 96.69%\n","Epoch [8/10], Step [34/39], Loss: 0.0398, Accuracy: 96.78%\n","Epoch [8/10], Step [35/39], Loss: 0.0400, Accuracy: 96.88%\n","Epoch [8/10], Step [36/39], Loss: 0.0420, Accuracy: 96.79%\n","Epoch [8/10], Step [37/39], Loss: 0.0424, Accuracy: 96.88%\n","Epoch [8/10], Step [38/39], Loss: 0.0427, Accuracy: 96.96%\n","Epoch [8/10], Step [39/39], Loss: 0.0432, Accuracy: 96.98%\n","Epoch [9/10], Step [1/39], Loss: 0.0004, Accuracy: 100.00%\n","Epoch [9/10], Step [2/39], Loss: 0.0006, Accuracy: 100.00%\n","Epoch [9/10], Step [3/39], Loss: 0.0013, Accuracy: 98.96%\n","Epoch [9/10], Step [4/39], Loss: 0.0018, Accuracy: 98.44%\n","Epoch [9/10], Step [5/39], Loss: 0.0020, Accuracy: 98.75%\n","Epoch [9/10], Step [6/39], Loss: 0.0028, Accuracy: 98.44%\n","Epoch [9/10], Step [7/39], Loss: 0.0030, Accuracy: 98.66%\n","Epoch [9/10], Step [8/39], Loss: 0.0033, Accuracy: 98.83%\n","Epoch [9/10], Step [9/39], Loss: 0.0035, Accuracy: 98.96%\n","Epoch [9/10], Step [10/39], Loss: 0.0037, Accuracy: 99.06%\n","Epoch [9/10], Step [11/39], Loss: 0.0039, Accuracy: 99.15%\n","Epoch [9/10], Step [12/39], Loss: 0.0044, Accuracy: 98.96%\n","Epoch [9/10], Step [13/39], Loss: 0.0046, Accuracy: 99.04%\n","Epoch [9/10], Step [14/39], Loss: 0.0049, Accuracy: 99.11%\n","Epoch [9/10], Step [15/39], Loss: 0.0060, Accuracy: 98.96%\n","Epoch [9/10], Step [16/39], Loss: 0.0070, Accuracy: 98.63%\n","Epoch [9/10], Step [17/39], Loss: 0.0079, Accuracy: 98.53%\n","Epoch [9/10], Step [18/39], Loss: 0.0084, Accuracy: 98.61%\n","Epoch [9/10], Step [19/39], Loss: 0.0087, Accuracy: 98.68%\n","Epoch [9/10], Step [20/39], Loss: 0.0092, Accuracy: 98.75%\n","Epoch [9/10], Step [21/39], Loss: 0.0098, Accuracy: 98.66%\n","Epoch [9/10], Step [22/39], Loss: 0.0107, Accuracy: 98.44%\n","Epoch [9/10], Step [23/39], Loss: 0.0123, Accuracy: 98.37%\n","Epoch [9/10], Step [24/39], Loss: 0.0125, Accuracy: 98.44%\n","Epoch [9/10], Step [25/39], Loss: 0.0133, Accuracy: 98.38%\n","Epoch [9/10], Step [26/39], Loss: 0.0135, Accuracy: 98.44%\n","Epoch [9/10], Step [27/39], Loss: 0.0154, Accuracy: 98.15%\n","Epoch [9/10], Step [28/39], Loss: 0.0159, Accuracy: 98.21%\n","Epoch [9/10], Step [29/39], Loss: 0.0160, Accuracy: 98.28%\n","Epoch [9/10], Step [30/39], Loss: 0.0165, Accuracy: 98.23%\n","Epoch [9/10], Step [31/39], Loss: 0.0167, Accuracy: 98.29%\n","Epoch [9/10], Step [32/39], Loss: 0.0168, Accuracy: 98.34%\n","Epoch [9/10], Step [33/39], Loss: 0.0170, Accuracy: 98.39%\n","Epoch [9/10], Step [34/39], Loss: 0.0172, Accuracy: 98.44%\n","Epoch [9/10], Step [35/39], Loss: 0.0173, Accuracy: 98.48%\n","Epoch [9/10], Step [36/39], Loss: 0.0174, Accuracy: 98.52%\n","Epoch [9/10], Step [37/39], Loss: 0.0189, Accuracy: 98.48%\n","Epoch [9/10], Step [38/39], Loss: 0.0191, Accuracy: 98.52%\n","Epoch [9/10], Step [39/39], Loss: 0.0193, Accuracy: 98.53%\n","Epoch [10/10], Step [1/39], Loss: 0.0015, Accuracy: 93.75%\n","Epoch [10/10], Step [2/39], Loss: 0.0016, Accuracy: 96.88%\n","Epoch [10/10], Step [3/39], Loss: 0.0021, Accuracy: 97.92%\n","Epoch [10/10], Step [4/39], Loss: 0.0027, Accuracy: 97.66%\n","Epoch [10/10], Step [5/39], Loss: 0.0031, Accuracy: 98.12%\n","Epoch [10/10], Step [6/39], Loss: 0.0046, Accuracy: 96.88%\n","Epoch [10/10], Step [7/39], Loss: 0.0054, Accuracy: 96.88%\n","Epoch [10/10], Step [8/39], Loss: 0.0058, Accuracy: 97.27%\n","Epoch [10/10], Step [9/39], Loss: 0.0058, Accuracy: 97.57%\n","Epoch [10/10], Step [10/39], Loss: 0.0059, Accuracy: 97.81%\n","Epoch [10/10], Step [11/39], Loss: 0.0062, Accuracy: 98.01%\n","Epoch [10/10], Step [12/39], Loss: 0.0063, Accuracy: 98.18%\n","Epoch [10/10], Step [13/39], Loss: 0.0067, Accuracy: 98.08%\n","Epoch [10/10], Step [14/39], Loss: 0.0073, Accuracy: 97.99%\n","Epoch [10/10], Step [15/39], Loss: 0.0073, Accuracy: 98.12%\n","Epoch [10/10], Step [16/39], Loss: 0.0075, Accuracy: 98.24%\n","Epoch [10/10], Step [17/39], Loss: 0.0077, Accuracy: 98.35%\n","Epoch [10/10], Step [18/39], Loss: 0.0087, Accuracy: 98.09%\n","Epoch [10/10], Step [19/39], Loss: 0.0089, Accuracy: 98.19%\n","Epoch [10/10], Step [20/39], Loss: 0.0091, Accuracy: 98.28%\n","Epoch [10/10], Step [21/39], Loss: 0.0093, Accuracy: 98.36%\n","Epoch [10/10], Step [22/39], Loss: 0.0094, Accuracy: 98.44%\n","Epoch [10/10], Step [23/39], Loss: 0.0100, Accuracy: 98.37%\n","Epoch [10/10], Step [24/39], Loss: 0.0103, Accuracy: 98.44%\n","Epoch [10/10], Step [25/39], Loss: 0.0107, Accuracy: 98.50%\n","Epoch [10/10], Step [26/39], Loss: 0.0110, Accuracy: 98.56%\n","Epoch [10/10], Step [27/39], Loss: 0.0110, Accuracy: 98.61%\n","Epoch [10/10], Step [28/39], Loss: 0.0111, Accuracy: 98.66%\n","Epoch [10/10], Step [29/39], Loss: 0.0113, Accuracy: 98.71%\n","Epoch [10/10], Step [30/39], Loss: 0.0114, Accuracy: 98.75%\n","Epoch [10/10], Step [31/39], Loss: 0.0115, Accuracy: 98.79%\n","Epoch [10/10], Step [32/39], Loss: 0.0116, Accuracy: 98.83%\n","Epoch [10/10], Step [33/39], Loss: 0.0123, Accuracy: 98.77%\n","Epoch [10/10], Step [34/39], Loss: 0.0124, Accuracy: 98.81%\n","Epoch [10/10], Step [35/39], Loss: 0.0125, Accuracy: 98.84%\n","Epoch [10/10], Step [36/39], Loss: 0.0128, Accuracy: 98.87%\n","Epoch [10/10], Step [37/39], Loss: 0.0131, Accuracy: 98.90%\n","Epoch [10/10], Step [38/39], Loss: 0.0137, Accuracy: 98.85%\n","Epoch [10/10], Step [39/39], Loss: 0.0142, Accuracy: 98.86%\n","Finished Training\n","Accuracy of the model on the test images: 91.18%\n"]}],"source":["# Train and Evaluate the Model\n","train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=10)\n","accuracy = evaluate_model(model, test_loader)"]},{"cell_type":"code","execution_count":10,"id":"c50102e7","metadata":{"id":"c50102e7","executionInfo":{"status":"ok","timestamp":1727028252262,"user_tz":-480,"elapsed":17,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}}},"outputs":[],"source":["# save model to Google Drive\n","model_save_name = 'flower_model.pt'\n","path = F\"/content/drive/MyDrive/{model_save_name}\"\n","torch.save(model.state_dict(), path)"]},{"cell_type":"code","source":["accuracy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Danol99H6Uw2","executionInfo":{"status":"ok","timestamp":1727028359770,"user_tz":-480,"elapsed":386,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}},"outputId":"68dd5a29-114e-4db2-df68-efc887714221"},"id":"Danol99H6Uw2","execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["91.17647058823529"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","id":"8b242f0b-4b30-4b08-a310-b5005e120eb8","metadata":{"id":"8b242f0b-4b30-4b08-a310-b5005e120eb8"},"source":["### 2.2 Submission\n","\n","You'll submit the link to your model and the accuracy score for your model in this section.\n","\n","Remember to download or move/upload your model from Google Colab to your Google Drive. Put the link to the Google Drive for your model below.\n","\n","If you use a different variable to store your accuracy, remember to also change the `accuracy` variable below."]},{"cell_type":"code","execution_count":12,"id":"2e4d6774-0c27-48ec-abcc-ba9747ad1dc0","metadata":{"id":"2e4d6774-0c27-48ec-abcc-ba9747ad1dc0","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1727028370780,"user_tz":-480,"elapsed":2834,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}},"outputId":"28f312c6-babf-4a23-f2c3-6ccd78f27933"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Assignment successfully submitted'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}],"source":["# Submit Method\n","model_link = \"https://drive.google.com/file/d/1bQBoQxtfjKPEjoinZQC02uSXYbCmTooZ/view?usp=sharing\" # Put your model link\n","\n","question_id = \"01_cnn_project_model_link\"\n","submit(student_id, name, assignment_id, model_link, question_id, drive_link)\n","question_id = \"02_cnn_project_model_accuracy\"\n","submit(student_id, name, assignment_id, str(accuracy), question_id, drive_link)"]},{"cell_type":"markdown","id":"156bff33-3226-4366-8ea0-df0b788f9861","metadata":{"id":"156bff33-3226-4366-8ea0-df0b788f9861"},"source":["## Task-3 Model Inference\n","\n","In this task, you will be exercising the application of your model, or as it's commonly referred to in AI terminology, you will be performing inference using your model.\n","\n","Simply load your saved model from Task-2 and create an inference for the model. Where you'll feed an image as input and the model will output the label as well as the percentage of confidence for the label."]},{"cell_type":"markdown","id":"cd2313b4-c6f6-4305-be72-f21cdf531132","metadata":{"id":"cd2313b4-c6f6-4305-be72-f21cdf531132"},"source":["### 3.1 Write your code in the block below\n","\n","In the code block below write the code to use the model you created in Task-2. Load the model and input image, afterwards, show the result of the label/class together with confidence level in percentage as well as the input image."]},{"cell_type":"code","execution_count":13,"id":"c9120366-86dc-4314-ba9d-66f60ae5a884","metadata":{"id":"c9120366-86dc-4314-ba9d-66f60ae5a884","executionInfo":{"status":"ok","timestamp":1727028437373,"user_tz":-480,"elapsed":394,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}}},"outputs":[],"source":["#Write your code for inference here\n","# Define class names (label for the 15 types of flowers)\n","class_names = ['Bluebell', 'Buttercup', 'Cowslip', 'Crocus', 'Daffodil', 'Daisy', 'Dandelion',\n","               'Fritillary', 'Iris', 'Pansy', 'Snowdrop', 'Sunflower', 'Tiger lily', 'Tulip',\n","               'Windflower']\n","\n","# Function to make prediction using Gradio\n","def predict(image):\n","    # Preprocess image\n","    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension and send to GPU if available\n","\n","    model.eval()\n","    with torch.no_grad():\n","        output = model(image)\n","        _, predicted = torch.max(output, 1)\n","        label = class_names[predicted.item()]\n","\n","    return label\n","\n","# Gradio Interface\n","interface = gr.Interface(\n","    fn=predict,  # Prediction function\n","    inputs=gr.Image(type=\"pil\"),  # Input type is an image (uploaded by user)\n","    outputs=\"text\",  # Output is the predicted label (flower type)\n","    title=\"Flower Classification\",\n","    description=\"Upload an image of a flower, and the model will predict the type of flower.\"\n",")\n","\n"]},{"cell_type":"code","source":["interface.launch()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":625},"id":"bJCiM1gn6nlY","executionInfo":{"status":"ok","timestamp":1727028442065,"user_tz":-480,"elapsed":1144,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}},"outputId":"4eaf8f9b-401e-4d40-c9d7-22bc3cf3502b"},"id":"bJCiM1gn6nlY","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://25e8761c51acff2789.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://25e8761c51acff2789.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","id":"2c39805e-a94c-4470-af4e-7669ff33f84f","metadata":{"id":"2c39805e-a94c-4470-af4e-7669ff33f84f"},"source":["### 3.2 Submission\n","\n","You'll submit a screenshot of your inference in this section. Remember to save the screenshot first before submitting it.\n","\n","Hint:\n","\n","![Upload colab](https://storage.googleapis.com/rg-ai-bootcamp/project-3-pipeline-and-gradio/upload-colab.png)\n","\n","- In Google Colab you can just use the \"Folder\" sidebar and click the upload button. Make sure your screenshot match below requirements:\n","\n","    - Image name screenshot is `submission.jpg`. If you change the name of the screenshot  file, change it also in the submit_image parameter.\n","    - The input image and label as well as percentage of confidence should be included in the screenshot\n","\n","Here is an example of a correct screenshot:\n","\n","![Screenshot submission sammple - hummer](https://storage.googleapis.com/rg-ai-bootcamp/projects/project-5-cnn-hummer.png)"]},{"cell_type":"code","execution_count":15,"id":"7f621399-107e-494f-8c6f-9a79f4978d5d","metadata":{"id":"7f621399-107e-494f-8c6f-9a79f4978d5d","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1727028592176,"user_tz":-480,"elapsed":6591,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}},"outputId":"611f4d6a-7235-4387-d6ea-1ea00e1127ad"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Assignment successfully submitted'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["# Submit Method\n","\n","question_id = \"03_cnn_project_inference\"\n","submit_image(student_id, question_id, './submission.jpg')"]},{"cell_type":"markdown","id":"2a021cd5-9c96-48d9-834d-18d37ba76824","metadata":{"id":"2a021cd5-9c96-48d9-834d-18d37ba76824"},"source":["## Task-4 Model Publishing\n","\n","In this task, you will push your model to Huggingface. Once you've pushed your model to the Hugging Face Model Hub, you'll have a link that points directly to your model's page. You can share this link with others, and they can use it to directly load your model for their own uses."]},{"cell_type":"markdown","id":"f9cbd245-e9f7-4fdb-bc2c-1ca8c63af516","metadata":{"id":"f9cbd245-e9f7-4fdb-bc2c-1ca8c63af516"},"source":["### 4.1 Write your code in the block below\n","\n","In the code block below, write the code to push your model to Huggingface. There are several methods to do this, please refer to the documentation: https://huggingface.co/docs/transformers/model_sharing\n","\n","Some techniques you may use:\n","- If you use the Transformer Trainer during the training loop when you create your model above, then you can simply put your `trainer.push_to_hub()` here.\n","- You can also use the web interface on Huggingface.\n","\n","Hint:\n","- Remember to login first to your Huggingface account.\n","- If you are pushing programmaticaly, then use the huggingface-cli to login."]},{"cell_type":"code","execution_count":16,"id":"ca04df60-d131-4108-88ae-18c253cfa1ba","metadata":{"id":"ca04df60-d131-4108-88ae-18c253cfa1ba","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727028683589,"user_tz":-480,"elapsed":62139,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}},"outputId":"1f9dffa3-e5b2-41ef-849a-b915e2be8044"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["#Write your code for publishing here\n","!huggingface-cli login\n"]},{"cell_type":"code","execution_count":30,"id":"15fcc782","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["c1367f5fb6d048f4afb1d80c465c7796","36a277a9779b4b50a5b4bce8ca90182e","c137317d4cf04f15bff751bdede89354","ac9da9615b2d42c6a65d4d78351f702e","954f9e7ea9fe44c5b19d6b8457dd0de8","e72eea09fd624444a134d2f3af08787a","00cb3911e4394e3ebf1c0605ad3ffbbf","f3125d8ed6d94bb7ab94c544dd9eb665","3daa15edb61a404a82ce2301e5031ca5","71d6814c013040f483a9e729e239a170","151c828fa4b846b48a2459ff04beea8f"]},"id":"15fcc782","executionInfo":{"status":"ok","timestamp":1727030562559,"user_tz":-480,"elapsed":3180,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}},"outputId":"ec3205f0-33f2-4402-eb80-8eaaf2612ca2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["resnet_model.pth:   0%|          | 0.00/44.8M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1367f5fb6d048f4afb1d80c465c7796"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/fathurim/15OxfordFlower/commit/5f48dd374c91bc4131ef3303aed73bc129c2ad40', commit_message='Upload model.bin with huggingface_hub', commit_description='', oid='5f48dd374c91bc4131ef3303aed73bc129c2ad40', pr_url=None, pr_revision=None, pr_num=None)"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}],"source":["\n","from huggingface_hub import HfApi\n","\n","api = HfApi()\n","api.upload_file(\n","    path_or_fileobj=\"/content/resnet_model.pth\",\n","    path_in_repo=\"model.bin\",  # or your preferred filename\n","    repo_id=\"fathurim/15OxfordFlower\",\n","    repo_type=\"model\"\n",")\n"]},{"cell_type":"markdown","id":"e8dc66c0-2b13-46c0-9be6-2835917df550","metadata":{"id":"e8dc66c0-2b13-46c0-9be6-2835917df550"},"source":["### 4.2 Submission\n","\n","You'll submit a a link to your huggingface model in this section.\n","\n","The following link is an example of what a trained model's page looks like: https://huggingface.co/aditira/emotion_classification. This is not your model, but rather an example of what your final result might resemble.\n","\n","Remember, for this project you should push your output model to your own Hugging Face account. The link for your model will be different and should reflect your own username and model name."]},{"cell_type":"code","execution_count":31,"id":"51d45a76-e657-4eb9-bdc4-9526fac166e9","metadata":{"id":"51d45a76-e657-4eb9-bdc4-9526fac166e9","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1727030716667,"user_tz":-480,"elapsed":1693,"user":{"displayName":"Fathur Imam Mujaddid","userId":"00438109393749372913"}},"outputId":"9e3e7337-a687-4b40-d5ba-f96e6df5ae56"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Assignment successfully submitted'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}],"source":["# Submit Method\n","huggingface_model_link = \"https://huggingface.co/fathurim/15OxfordFlower/commit/5f48dd374c91bc4131ef3303aed73bc129c2ad40\" # Put your model link\n","\n","question_id = \"04_cnn_project_publish\"\n","submit(student_id, name, assignment_id, huggingface_model_link, question_id, drive_link)"]},{"cell_type":"markdown","id":"aea843c0-2811-416f-af2d-a4e46a65a11a","metadata":{"id":"aea843c0-2811-416f-af2d-a4e46a65a11a"},"source":["FIN"]}],"metadata":{"colab":{"provenance":[{"file_id":"1x_JwfHr0uYbUtXSQKRzYxGS1mmzHcZu3","timestamp":1727025838764}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c1367f5fb6d048f4afb1d80c465c7796":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_36a277a9779b4b50a5b4bce8ca90182e","IPY_MODEL_c137317d4cf04f15bff751bdede89354","IPY_MODEL_ac9da9615b2d42c6a65d4d78351f702e"],"layout":"IPY_MODEL_954f9e7ea9fe44c5b19d6b8457dd0de8"}},"36a277a9779b4b50a5b4bce8ca90182e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e72eea09fd624444a134d2f3af08787a","placeholder":"​","style":"IPY_MODEL_00cb3911e4394e3ebf1c0605ad3ffbbf","value":"resnet_model.pth: 100%"}},"c137317d4cf04f15bff751bdede89354":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3125d8ed6d94bb7ab94c544dd9eb665","max":44816308,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3daa15edb61a404a82ce2301e5031ca5","value":44816308}},"ac9da9615b2d42c6a65d4d78351f702e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71d6814c013040f483a9e729e239a170","placeholder":"​","style":"IPY_MODEL_151c828fa4b846b48a2459ff04beea8f","value":" 44.8M/44.8M [00:01&lt;00:00, 19.2MB/s]"}},"954f9e7ea9fe44c5b19d6b8457dd0de8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e72eea09fd624444a134d2f3af08787a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00cb3911e4394e3ebf1c0605ad3ffbbf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3125d8ed6d94bb7ab94c544dd9eb665":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3daa15edb61a404a82ce2301e5031ca5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71d6814c013040f483a9e729e239a170":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"151c828fa4b846b48a2459ff04beea8f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}